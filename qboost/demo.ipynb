{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QBoost: Binary Classification with a Quantum Computer\n",
    "\n",
    "The D-Wave quantum computer has been widely studied as a discrete optimization engine that accepts any problem formulated as quadratic unconstrained  binary  optimization  (QUBO). In 2008, Google and D-Wave published a paper, [Training a Binary Classifier with the Quantum Adiabatic Algorithm](https://arxiv.org/pdf/0811.0416.pdf), which describes how the `QBoost` ensemble method makes binary classification amenable to quantum computing: the problem is formulated as a thresholded linear superposition of a set of weak classifiers and the D-Wave quantum computer is  used to optimize the weights in a learning process that strives to minimize the training error and number of weak classifiers.\n",
    "\n",
    "This notebook demonstrates and explains how the QBoost algorithm can be used to solve a binary classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of data as shown below. We want to divide the data into two sets - can you see a clear delineation of the two sets based on the pattern of the data?\n",
    "\n",
    "![Unclassified_Training_Set](images/DataSet_Unclassified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've figured out what our two sets are, the dividing line between them is a $\\textbf{classifier}$. A classifier can help us to determine which set new data points might belong to.  For example, which set do you think each of the data points below belongs to?\n",
    "\n",
    "![Unclassified_Test_Set](images/TrainingData_Unclassified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different algorithms available for building a classifier.  In this notebook we will explore both CPU-based algorithms (AdaBoost, Decision Trees, and Random Forest) and QPU-based algorithms (QBoost and QBoostPlus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up our initial data set, we will use the tools available in the  `sklearn` library.  We generate a set of training data (items with labels in $\\{-1, 1\\}$) and a set of test data (items with labels in $\\{-1,1\\}$).  We will use the training data to build each classifier, then use the test data to compare the classifier-produced labels with the provided known test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X1, Y1 = make_blobs(n_samples=15000, n_features=2, centers=2, random_state=1)\n",
    "Y1 = 2*Y1-1\n",
    "\n",
    "X_train = X1[0:9999]\n",
    "y_train = Y1[0:9999]\n",
    "X_test = X1[10000:14999]\n",
    "y_test = Y1[10000:14999]\n",
    "\n",
    "plt.title(\"Training Data\", fontsize='small')\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Test Data\", fontsize='small')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='x', color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "A decision tree uses a tree structure to classify the data.  It uses the non-leaf nodes to map the data to a set of  decision rules and leaf nodes to determine labels for each data item. For more information on decision trees as classifiers, check out [scikit-learn's page](http://scikit-learn.org/stable/modules/tree.html#tree-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree(X_train, y_train, X_test, y_test):\n",
    "    from sklearn import tree\n",
    "\n",
    "    clf1 = tree.DecisionTreeClassifier()\n",
    "    clf1.fit(X_train, y_train)\n",
    "    y_train1 = clf1.predict(X_train)\n",
    "    y_test1 = clf1.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    print('Accuracy for training data: \\t', (accuracy_score(y_train, y_train1)))\n",
    "    print('Accuracy for test data: \\t', (accuracy_score(y_test, y_test1)))\n",
    "    \n",
    "    return clf1\n",
    "    \n",
    "clf1 = Decision_Tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows us the accuracy of the classifier that we built on our training data and on our test data.  Note that these are percentages, so a score of 1.00 would indicate that 100% of our data in the given set is labeled correctly using the classifier that we generated.\n",
    "\n",
    "These great results are expected because of the nature of this simple data set - we would not expect to see results this good on real-world data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forest is an ensemble method, which combines several weaker classifiers to create one strong classifier. It typically uses a set of decision trees as weak classifiers that are individually weighted. By introducing randomness into the underlying decision trees, the ensemble diversifies the weightings of its collection of weak classifiers, generally resulting in an improved model. For more information random forests as classifiers, check out [scikit-learn's page.](http://scikit-learn.org/stable/modules/ensemble.html#forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf2 = RandomForestClassifier(max_depth=2, n_estimators=30)\n",
    "    clf2.fit(X_train, y_train)\n",
    "    y_train2 = clf2.predict(X_train)\n",
    "    y_test2 = clf2.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    print('Accuracy for training data: \\t', (accuracy_score(y_train, y_train2)))\n",
    "    print('Accuracy for test data: \\t', (accuracy_score(y_test, y_test2)))\n",
    "    \n",
    "    return clf2\n",
    "\n",
    "clf2 = Random_Forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "AdaBoost is an ensemble method in which a classifer is constructed in an iterative fashion. In each iteration, one weak classifier is selected and re-learned to minimize a weighted error function.  The final classification model will be decided by a weighted “vote” of all the weak classifiers.  The scikit-learn package implements its AdaBoost method with decision trees of depth 1, also known as tree stumps.  For more information on AdaBoost, check out [scikit-learn's page.](http://scikit-learn.org/stable/modules/ensemble.html#adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "    clf3 = AdaBoostClassifier(n_estimators=30)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    y_train3 = clf3.predict(X_train)\n",
    "    y_test3 = clf3.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    print('Accuracy for training data: \\t', (accuracy_score(y_train, y_train3)))\n",
    "    print('Accuracy for test data: \\t', (accuracy_score(y_test, y_test3)))\n",
    "    \n",
    "    return clf3\n",
    "    \n",
    "clf3 = AdaBoost(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QBoost\n",
    "\n",
    "Like AdaBoost, QBoost is an ensemble method.  To make use of the optimization power of D-Wave quantum annealer, we need to formulate a quadratic unconstrained binary optimization (QUBO) objective function. To do this, we modify AdaBoost by replacing the traditional weighted error function with a QUBO.  Be sure to enter your D-Wave token into the `my_token` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QBoost(X_train, y_train, X_test, y_test):\n",
    "    NUM_READS = 1000\n",
    "    DW_PARAMS = {'num_reads': NUM_READS,\n",
    "                 'auto_scale': True,\n",
    "                 'num_spin_reversal_transforms': 10,\n",
    "                 'postprocess': 'optimization',\n",
    "                 }\n",
    "\n",
    "    from dwave.system.samplers import DWaveSampler\n",
    "    from dwave.system.composites import EmbeddingComposite\n",
    "\n",
    "    dwave_sampler = DWaveSampler(solver={'qpu': True}) # Some accounts need to replace this line with the next:\n",
    "    # dwave_sampler = DWaveSampler(token='ENTER TOKEN HERE', solver='ENTER SOLVER NAME HERE')\n",
    "    emb_sampler = EmbeddingComposite(dwave_sampler)\n",
    "\n",
    "    from qboost import WeakClassifiers, QBoostClassifier\n",
    "\n",
    "    clf4 = QBoostClassifier(n_estimators=30, max_depth=2)\n",
    "    clf4.fit(X_train, y_train, emb_sampler, lmd=1.0, **DW_PARAMS)\n",
    "    y_train4 = clf4.predict(X_train)\n",
    "    y_test4 = clf4.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    print('Accuracy for training data: \\t', (accuracy_score(y_train, y_train4)))\n",
    "    print('Accuracy for test data: \\t', (accuracy_score(y_test, y_test4)))\n",
    "    \n",
    "    return clf4\n",
    "    \n",
    "clf4 = QBoost(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QBoostPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QBoostPlus uses all of our previous classifiers to generate a new classifier.  You must run all of the previous classifiers before running QBoostPlus.  Be sure to enter your D-Wave token into the `my_token` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QBoostPlus(X_train, y_train, X_test, y_test, clf1, clf2, clf3, clf4):\n",
    "    NUM_READS = 1000\n",
    "    DW_PARAMS = {'num_reads': NUM_READS,\n",
    "                 'auto_scale': True,\n",
    "                 'num_spin_reversal_transforms': 10,\n",
    "                 'postprocess': 'optimization',\n",
    "                 }\n",
    "\n",
    "    from dwave.system.samplers import DWaveSampler\n",
    "    from dwave.system.composites import EmbeddingComposite\n",
    "\n",
    "    dwave_sampler = DWaveSampler(solver={'qpu': True}) # Some accounts need to replace this line with the next:\n",
    "    # dwave_sampler = DWaveSampler(token='ENTER TOKEN HERE', solver='ENTER SOLVER NAME HERE')\n",
    "    emb_sampler = EmbeddingComposite(dwave_sampler)\n",
    "    \n",
    "    from qboost import QboostPlus\n",
    "\n",
    "    clf5 = QboostPlus([clf1, clf2, clf3, clf4])\n",
    "    clf5.fit(X_train, y_train, emb_sampler, lmd=0.2, **DW_PARAMS)\n",
    "    y_train5 = clf5.predict(X_train)\n",
    "    y_test5 = clf5.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    print('Accuracy for training data: \\t', (accuracy_score(y_train, y_train5)))\n",
    "    print('Accuracy for test data: \\t', (accuracy_score(y_test, y_test5)))\n",
    "    \n",
    "    return clf5\n",
    "    \n",
    "clf5 = QBoostPlus(X_train, y_train, X_test, y_test, clf1, clf2, clf3, clf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Now we're ready to run some experiments on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment 1: Binary Classfication on the MNIST Dataset \n",
    "This example transforms the MNIST dataset (handwritten digits) into a binary classification problem. We assume all digits that are smaller than 5 are labelled as -1 and the rest digits are labelled as +1.\n",
    "\n",
    "First, let us load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "\n",
    "# Loading the data set\n",
    "mnist = fetch_mldata('MNIST original', data_home='data')\n",
    "\n",
    "# Gathering the indices for the data labelled with numbers <= 9\n",
    "idx_01 = np.where(mnist.target <= 9)[0]\n",
    "\n",
    "# Shuffling the data for a random selection for training and test data\n",
    "np.random.shuffle(idx_01)\n",
    "\n",
    "# Selecting 15,000 items for our total data set\n",
    "idx_01 = idx_01[:15000]\n",
    "\n",
    "# Using 2/3 of our data set for training, 1/3 for testing\n",
    "idx_train = idx_01[:2*len(idx_01)//3]\n",
    "idx_test = idx_01[2*len(idx_01)//3:]\n",
    "\n",
    "# Setting up the data points for training and testing\n",
    "X_train = mnist.data[idx_train]\n",
    "X_test = mnist.data[idx_test]\n",
    "\n",
    "# Setting up the labels for training and testing.  Labels should be -1, +1 for QBoost and QBoostPlus.\n",
    "y_train = 2*(mnist.target[idx_train] >4) - 1\n",
    "y_test = 2*(mnist.target[idx_test] >4) - 1\n",
    "\n",
    "print(\"Training data size: \\t%d samples with %d features\" %(X_train.shape[0], X_train.shape[1]))\n",
    "print(\"Testing data size: \\t%d samples\" %(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the digits: digits with class $+1$ are shown as images with a black background while digits with class $-1$  as images with a white background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(16):\n",
    "    if y_train[i] == 1:\n",
    "        COLORMAP = 'gray'\n",
    "    else:\n",
    "        COLORMAP = 'gray_r'\n",
    "    plt.subplot(4,4, i+1)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap=COLORMAP)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model and compare the results of the selected classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=======================================')\n",
    "# Decision Tree\n",
    "print('Decision Tree: ')\n",
    "clf1 = Decision_Tree(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# Random Forest\n",
    "print('Random Forest: ')\n",
    "clf2 = Random_Forest(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# AdaBoost\n",
    "print('AdaBoost: ')\n",
    "clf3 = AdaBoost(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# QBoost\n",
    "print('QBoost: ')\n",
    "clf4 = QBoost(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# QBoostPlus\n",
    "print('QBoostPlus: ')\n",
    "clf5 = QBoostPlus(X_train, y_train, X_test, y_test, clf1, clf2, clf3, clf4)\n",
    "print('=======================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiment 2: Wisconsin Breast Cancer\n",
    "\n",
    "This example classifies tumors in scikit-learn's Wisconsin breast cancer dataset as either malignant or benign (binary classification).\n",
    "\n",
    "First, let us load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Loading the data set\n",
    "wisc = load_breast_cancer()\n",
    "\n",
    "# Shuffling the data for a random selection for training and test data\n",
    "idx = np.arange(len(wisc.target))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Using 2/3 of our data set for training, 1/3 for testing\n",
    "idx_train = idx[:2*len(idx)//3]\n",
    "idx_test = idx[2*len(idx)//3:]\n",
    "\n",
    "# Setting up the data points for training and testing\n",
    "X_train = wisc.data[idx_train]\n",
    "X_test = wisc.data[idx_test]\n",
    "\n",
    "# Setting up the labels for training and testing.  Labels should be -1, +1 for QBoost and QBoostPlus.\n",
    "y_train = 2 * wisc.target[idx_train] - 1  \n",
    "y_test = 2 * wisc.target[idx_test] - 1\n",
    "\n",
    "print(\"Training data size: \\t%d samples with %d features\" %(X_train.shape[0], X_train.shape[1]))\n",
    "print(\"Testing data size: \\t%d samples\" %(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model and compare the results of the selected classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=======================================')\n",
    "# Decision Tree\n",
    "print('Decision Tree: ')\n",
    "clf1 = Decision_Tree(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# Random Forest\n",
    "print('Random Forest: ')\n",
    "clf2 = Random_Forest(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# AdaBoost\n",
    "print('AdaBoost: ')\n",
    "clf3 = AdaBoost(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# QBoost\n",
    "print('QBoost: ')\n",
    "clf4 = QBoost(X_train, y_train, X_test, y_test)\n",
    "print('---------------------------------------')\n",
    "# QBoostPlus\n",
    "print('QBoostPlus: ')\n",
    "clf5 = QBoostPlus(X_train, y_train, X_test, y_test, clf1, clf2, clf3, clf4)\n",
    "print('=======================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3:  Try it Yourself\n",
    "\n",
    "In the block below, follow the prompts to import a dataset from scikit-learn and try building the different classifiers on the data set.\n",
    "\n",
    "We will use scikit-learn's wine data set.  This data set is divided into classes 0, 1, and 2.  We will work to classify the sets {class 0, class 1} and {class 2}.  Use the examples in Experiment 1 and Experiment 2 to fill in this code outline and classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# import numpy to work with numpy arrays\n",
    "\n",
    "\n",
    "# Import scikit-learn's wine data set library.\n",
    "\n",
    "\n",
    "# Load the scikit-learn's wine data set.\n",
    "\n",
    "\n",
    "# Gather the indices of the data we want to use from the dataset (there is a lot more than just points and labels!).\n",
    "\n",
    "\n",
    "# Shuffle the data for a random selection for training and test data.\n",
    "\n",
    "\n",
    "# Divide the data into 2/3 for training, 1/3 for testing\n",
    "\n",
    "\n",
    "# Set up the data points for training and testing\n",
    "\n",
    "\n",
    "# Set up the labels for training and testing.  Labels should be -1, +1 for QBoost and QBoostPlus.  \n",
    "# Remember we need classes 0 and 1 to map to set -1 and class 2 to map to set +1.\n",
    "\n",
    "\n",
    "# Run the different classifiers we have set up in this notebook and compare their performance on this data set.\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "\n",
    "# QBoost\n",
    "\n",
    "\n",
    "# QBoostPlus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# import numpy to work with numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "# Import scikit-learn's wine data set library.\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the scikit-learn's wine data set.\n",
    "wine = load_wine()\n",
    "\n",
    "# Gather the indices of the data we want to use from the dataset (there is a lot more than just points and labels!).\n",
    "indices = np.where(wine.target <= 2)[0]\n",
    "\n",
    "# Shuffle the data for a random selection for training and test data.\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Divide the data into 2/3 for training, 1/3 for testing\n",
    "indices_train = indices[:2*len(indices)//3]\n",
    "indices_test = indices[2*len(indices)//3:]\n",
    "\n",
    "# Set up the data points for training and testing\n",
    "X_train = wine.data[indices_train]\n",
    "X_test = wine.data[indices_test]\n",
    "\n",
    "# Set up the labels for training and testing.  Labels should be -1, +1 for QBoost and QBoostPlus.  \n",
    "# Remember we need classes 0 and 1 to map to set -1 and class 2 to map to set +1.\n",
    "y_train = 2*(wine.target[indices_train] > 1) - 1\n",
    "y_test = 2*(wine.target[indices_test] > 1) - 1\n",
    "\n",
    "print(\"Training data size: \\t%d samples with %d features\" %(X_train.shape[0], X_train.shape[1]))\n",
    "print(\"Testing data size: \\t%d samples\" %(X_test.shape[0]))\n",
    "\n",
    "# Run the different classifiers we have set up in this notebook and compare their performance on this data set.\n",
    "print('=======================================')\n",
    "# Decision Tree\n",
    "print('Decision Tree: ')\n",
    "clf1 = Decision_Tree(X_train, y_train, X_test, y_test) \n",
    "print('---------------------------------------')\n",
    "# Random Forest\n",
    "print('Random Forest: ')\n",
    "clf2 = Random_Forest(X_train, y_train, X_test, y_test) \n",
    "print('---------------------------------------')\n",
    "# AdaBoost\n",
    "print('AdaBoost: ')\n",
    "clf3 = AdaBoost(X_train, y_train, X_test, y_test) \n",
    "print('---------------------------------------')\n",
    "# QBoost\n",
    "print('QBoost: ')\n",
    "clf4 = QBoost(X_train, y_train, X_test, y_test) \n",
    "print('---------------------------------------')\n",
    "# QBoostPlus\n",
    "print('QBoostPlus: ')\n",
    "clf5 = QBoostPlus(X_train, y_train, X_test, y_test, clf1, clf2, clf3, clf4) \n",
    "print('=======================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few More Words on Ensemble Methods\n",
    "\n",
    "Ensemble methods build a strong classifier (an improved model) by combining weak classifiers with the goal of:\n",
    "\n",
    "* decreasing variance (bagging)\n",
    "* decreasing bias (boosting)\n",
    "* improving prediction (voting)\n",
    "\n",
    "![Boosting Algorithm](images/boosting.jpg)\n",
    "\n",
    "### Bagging, Boosting, and Voting\n",
    "\n",
    "The ensemble method produces new training data sets by random sampling with replacement from the original set. In _bagging_, any element has the same probability to appear in a new dataset; in _boosting_, data elements are weighted before they are collected in the new dataset. Another distinction is that bagging is parallelizable but boosting has to be executed sequentially. You can learn more about the differences between these methods here: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/.\n",
    "\n",
    "Voting operates on labels only. Unlike boosting, the aggeragated classification performance is not used to further polish each weak classifier. Voting has two typical requirements of its collection of  weak classifiers: that there be __many__ and that they be __diverse__.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood:  Comparing AdaBoost and QBoost\n",
    "\n",
    "### AdaBoost\n",
    "AdaBoost combines a number of $N$ weak classifiers into a strong one as\n",
    "$$C(x) = sign\\left(\\sum_i^N w_i c_i(x)\\right),$$\n",
    "with $c_i(x) \\in [-1, +1]$ being the $i$-th weak classifier:\n",
    "\n",
    "$$c_i(x) = sign(w'*x + b)$$\n",
    "\n",
    "The loss function of AdaBoost is defined as\n",
    "$$\n",
    "L = \\sum_{n=1}^N \\exp\\left\\{ - y_n \\sum_{s=1}^S w_sc_k(x_n)\\right\\}.\n",
    "$$\n",
    "\n",
    "The strong classifier $C(\\cdot)$ is constructed in an iterative fashion. In each iteration, one weak classifier\n",
    "is selected and re-learned to minimize the weighted error function. Its weight is adjusted and renormalized to make sure the sum of all weights equals 1. \n",
    "\n",
    "The final classification model will be decided by a weighted “vote” of all the weak classifiers.\n",
    "\n",
    "### QBoost\n",
    "To create QBoost, we replace the exponential loss function in AdaBoost with the following quadratic loss function.\n",
    "$$\n",
    "w* = \\arg\\min_w\\left(\\sum_s \\left(\\frac{1}{N}\\sum_n^N w_nc_n(x_s) - y_s\\right)^2\\right) + \\lambda ||w||_0,\n",
    "$$\n",
    "where the regularization term is added to enable controlling of weight sparsity.\n",
    "\n",
    "Note in QBoost, the weight vector is binary.\n",
    "\n",
    "# Research Using QBoost\n",
    "For more information on how QBoost is appearing in published research, check out the following references.\n",
    "\n",
    "Boyda, Edward, et al. \"Deploying a quantum annealing processor to detect tree cover in aerial imagery of California.\" PloS one 12.2 (2017): e0172505. \n",
    "\n",
    "Denchev, Vasil S., et al. \"Robust classification with adiabatic quantum optimization.\" Proceedings of the 29th International Coference on International Conference on Machine Learning. Omnipress, 2012.\n",
    "\n",
    "Li, Richard Y., et al. \"Quantum annealing versus classical machine learning applied to a simplified computational biology problem.\" NPJ quantum information 4.1 (2018): 14.  \n",
    "\n",
    "Mott, Alex, et al. \"Solving a Higgs optimization problem with quantum annealing for machine learning.\" Nature 550.7676 (2017): 375.\n",
    "\n",
    "Neven, H., et al. \"Binary classification using hardware implementation of quantum annealing Demonstrations at NIPS-09.\" 24th Annual Conf. on Neural Information Processing Systems. 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading your Own Dataset\n",
    "\n",
    "If you're interesting in trying out these classifiers on your own data set, check out [scikit-learn's page](http://scikit-learn.org/stable/datasets/index.html#external-datasets) on loading external datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
